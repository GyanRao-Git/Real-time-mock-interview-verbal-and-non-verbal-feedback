{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tA2gV-LyGYj",
        "outputId": "7b0ca968-7b4c-41ad-b5bb-b79fc6437b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 160MB/s]\n"
          ]
        }
      ],
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Unzip AffectNet if needed\n",
        "import zipfile, os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/affectnet.zip'  # <- Update if needed\n",
        "extract_path = '/content/affectnet'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "# 3. Imports\n",
        "import torch, torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 4. Dataset Class with Face Cropping using YOLO labels\n",
        "class AffectNetDataset(Dataset):\n",
        "    def __init__(self, images_dir, labels_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.labels_dir = labels_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted([\n",
        "            f for f in os.listdir(images_dir)\n",
        "            if f.endswith('.jpg') or f.endswith('.png')\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        width, height = image.size\n",
        "\n",
        "        # Parse YOLO label\n",
        "        with open(label_path, 'r') as f:\n",
        "            line = f.readline().strip()\n",
        "            parts = line.split()\n",
        "            class_id = int(parts[0])\n",
        "            x_center = float(parts[1]) * width\n",
        "            y_center = float(parts[2]) * height\n",
        "            box_width = float(parts[3]) * width\n",
        "            box_height = float(parts[4]) * height\n",
        "\n",
        "            left = int(x_center - box_width / 2)\n",
        "            top = int(y_center - box_height / 2)\n",
        "            right = int(x_center + box_width / 2)\n",
        "            bottom = int(y_center + box_height / 2)\n",
        "\n",
        "            image = image.crop((left, top, right, bottom))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, class_id\n",
        "\n",
        "# 5. Image transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 6. Paths\n",
        "base_path = '/content/affectnet/YOLO_format'\n",
        "train_images = f'{base_path}/train/images'\n",
        "train_labels = f'{base_path}/train/labels'\n",
        "valid_images = f'{base_path}/valid/images'\n",
        "valid_labels = f'{base_path}/valid/labels'\n",
        "test_images = f'{base_path}/test/images'\n",
        "test_labels = f'{base_path}/test/labels'\n",
        "\n",
        "# 7. Datasets and loaders\n",
        "train_dataset = AffectNetDataset(train_images, train_labels, transform)\n",
        "valid_dataset = AffectNetDataset(valid_images, valid_labels, transform)\n",
        "test_dataset = AffectNetDataset(test_images, test_labels, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# 8. Model: ResNet18\n",
        "num_classes = 8  # Change this based on your dataset\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# 9. Loss & optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 10. Train/Val Functions\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        total_correct += (out.argmax(1) == y).sum().item()\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
        "\n",
        "def eval_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            total_correct += (out.argmax(1) == y).sum().item()\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
        "\n",
        "#training loop\n",
        "# Initialize lists to store metrics\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = eval_model(model, valid_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Plot loss curves\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy curves\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1, epochs+1), train_accs, label='Train Accuracy')\n",
        "plt.plot(range(1, epochs+1), val_accs, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Final test accuracy\n",
        "test_loss, test_acc = eval_model(model, test_loader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmf7hh4KhYYH",
        "outputId": "7e92a592-f4f8-45f2-cc92-e355160d4bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/MyDrive/affectnet_resnet18.pth\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model (assuming training cell was already run)\n",
        "save_path = '/content/drive/MyDrive/affectnet_resnet18.pth'\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPDL1LyfAFug",
        "outputId": "ba5f7a3a-3cec-4bec-ac1c-5218c4578b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to ZIP\n",
        "zip_path = '/content/drive/MyDrive/ravdess.zip'  # change if needed\n",
        "extract_path = '/content/ravdess'\n",
        "\n",
        "# Extract if not already done\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M--U2nHzfe-d",
        "outputId": "bdbbbab9-b44a-4325-b8d5-506051401793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total audio files: 1440\n",
            "Sample file: /content/ravdess/Actor_23/03-01-04-01-02-01-23.wav\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "wav_files = glob.glob(os.path.join(extract_path, 'Actor_*', '*.wav'))\n",
        "print(f'Total audio files: {len(wav_files)}')\n",
        "print(f'Sample file: {wav_files[0]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCdK0Ni0fiqZ"
      },
      "outputs": [],
      "source": [
        "def extract_emotion(file_path):\n",
        "    file_name = os.path.basename(file_path)\n",
        "    emotion_id = int(file_name.split('-')[2])\n",
        "    return emotion_id - 1  # Convert to 0-based indexing (0–7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwjXPkxofjzr"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(file_path, n_mfcc=40):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
        "    return mfccs_mean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR0IhfKKfnkf"
      },
      "outputs": [],
      "source": [
        "X, y = [], []\n",
        "\n",
        "for file_path in wav_files:\n",
        "    features = extract_features(file_path)\n",
        "    label = extract_emotion(file_path)\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(\"Feature shape:\", X.shape, \"Label shape:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BonY-qhif7Eb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIgo6J43f-vM"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXBZ3Dj1gDgr"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "save_path = '/content/drive/MyDrive/ravdess_emotion_model.pkl'\n",
        "joblib.dump(clf, save_path)\n",
        "print(f\"Model saved to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/affectnet.zip\" -d \"/content/drive/MyDrive/affectnet_unzipped\"\n",
        "!ls \"/content/drive/MyDrive/affectnet_unzipped/YOLO_format/train/images\" | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq0LsrZfillU",
        "outputId": "55fbfe1c-72e4-42cd-d984-9b159237094d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffhq_0.png\n",
            "ffhq_1000.png\n",
            "ffhq_1001.png\n",
            "ffhq_1003.png\n",
            "ffhq_1004.png\n",
            "ffhq_1005.png\n",
            "ffhq_1007.png\n",
            "ffhq_1008.png\n",
            "ffhq_1009.png\n",
            "ffhq_100.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📂 Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📦 Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "import joblib\n",
        "import librosa\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# 💻 Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 🧠 Load AffectNet (ResNet18)\n",
        "affectnet_model = torch.hub.load('pytorch/vision:v0.14.0', 'resnet18', pretrained=False)\n",
        "affectnet_model.fc = nn.Linear(affectnet_model.fc.in_features, 8)\n",
        "affectnet_model.load_state_dict(torch.load('/content/drive/MyDrive/affectnet_resnet18.pth'))\n",
        "affectnet_model.to(device)\n",
        "affectnet_model.eval()\n",
        "\n",
        "# 🎧 Load RAVDESS Random Forest Model\n",
        "ravdess_model = joblib.load('/content/drive/MyDrive/ravdess_emotion_model.pkl')\n",
        "\n",
        "# 📊 Audio embedding function\n",
        "def get_ravdess_embedding(filepath):\n",
        "    y, sr = librosa.load(filepath, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    mfccs = mfccs.mean(axis=1)\n",
        "    pred = ravdess_model.predict_proba([mfccs])[0]\n",
        "    return torch.tensor(pred, dtype=torch.float32)\n",
        "\n",
        "# 🧷 AffectNet Dataset (YOLO format)\n",
        "class AffectNetYOLODataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.images = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt'))\n",
        "\n",
        "        with open(label_path, 'r') as f:\n",
        "            label = int(f.readline().split()[0])\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return 'affectnet', image, label\n",
        "\n",
        "# 🧷 RAVDESS Dataset\n",
        "class RAVDESSDataset(Dataset):\n",
        "    def __init__(self, base_dir, label_map):\n",
        "        self.samples = []\n",
        "        for root, _, files in os.walk(base_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.wav'):\n",
        "                    label = int(file.split('-')[2]) - 1  # Emotion ID is 3rd field (starts at 1)\n",
        "                    self.samples.append((os.path.join(root, file), label))\n",
        "        self.label_map = label_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        return 'ravdess', path, label\n",
        "\n",
        "# 🚀 Combine Datasets\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "affectnet_path = \"/content/drive/MyDrive/affectnet_unzipped/YOLO_format/train\"\n",
        "affectnet_dataset = AffectNetYOLODataset(\n",
        "    image_dir=os.path.join(affectnet_path, \"images\"),\n",
        "    label_dir=os.path.join(affectnet_path, \"labels\"),\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "\n",
        "ravdess_dataset = RAVDESSDataset(\"/content/drive/MyDrive/ravdess\", label_map=None)\n",
        "\n",
        "# 🧃 Merge into one dataset\n",
        "class FusionDataset(Dataset):\n",
        "    def __init__(self, affectnet_dataset, ravdess_dataset):\n",
        "        self.data = affectnet_dataset + ravdess_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "fusion_dataset = FusionDataset(affectnet_dataset, ravdess_dataset)\n",
        "fusion_loader = DataLoader(fusion_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# 🔁 Fusion Model\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, affectnet_model, ravdess_model_embedding_size=8, fusion_hidden_dim=128, num_classes=8):\n",
        "        super(FusionModel, self).__init__()\n",
        "        self.affectnet_model = affectnet_model\n",
        "        self.ravdess_embedding_size = ravdess_model_embedding_size\n",
        "\n",
        "        for param in self.affectnet_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.affectnet_model.fc = nn.Identity()\n",
        "\n",
        "        self.ravdess_fc = nn.Sequential(\n",
        "            nn.Linear(ravdess_model_embedding_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512 + 64, fusion_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(fusion_hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, source, x):\n",
        "        if source == 'affectnet':\n",
        "            emb = self.affectnet_model(x)\n",
        "            audio_emb = torch.zeros(x.size(0), self.ravdess_embedding_size).to(x.device)\n",
        "        else:\n",
        "            emb = torch.zeros(x.size(0), 512).to(x.device)\n",
        "            audio_emb = x\n",
        "\n",
        "        audio_emb = self.ravdess_fc(audio_emb)\n",
        "        fused = torch.cat((emb.view(emb.size(0), -1), audio_emb.view(audio_emb.size(0), -1)), dim=1)\n",
        "        return self.fusion_fc(fused)\n",
        "\n",
        "# 🧠 Train Fusion Model with Early Stopping\n",
        "fusion_model = FusionModel(affectnet_model).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=1e-4)\n",
        "num_epochs = 10\n",
        "early_stop_patience = 3\n",
        "best_loss = float('inf')\n",
        "no_improve = 0\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    fusion_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    progress_bar = tqdm(fusion_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for source, datas, labels in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        labels = labels.to(device)\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(len(source)):\n",
        "            if source[i] == 'affectnet':\n",
        "                img = datas[i].to(device)\n",
        "                out = fusion_model('affectnet', img.unsqueeze(0))\n",
        "            else:\n",
        "                emb = get_ravdess_embedding(datas[i]).to(device)\n",
        "                out = fusion_model('ravdess', emb.unsqueeze(0))\n",
        "            outputs.append(out)\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=0)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item(), \"Acc\": f\"{100 * correct / total:.2f}%\"})\n",
        "\n",
        "    avg_loss = epoch_loss / len(fusion_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"\\n✅ Epoch {epoch+1} | Loss: {avg_loss:.4f} | Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        no_improve = 0\n",
        "        torch.save(fusion_model.state_dict(), \"/content/best_fusion_model.pth\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= early_stop_patience:\n",
        "            print(\"🛑 Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# 📈 Plot Loss\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss per Epoch')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wcHdfFV3ihkk",
        "outputId": "622ee9eb-7363-4a9a-f6f9-f4c246afb9a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.14.0\n",
            "Epoch 1/10: 100%|██████████| 1521/1521 [04:43<00:00,  5.36it/s, Loss=1.79, Acc=16.68%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 1 | Loss: 1.9947 | Accuracy: 0.1668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 1521/1521 [02:36<00:00,  9.73it/s, Loss=2.06, Acc=17.15%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 2 | Loss: 1.9876 | Accuracy: 0.1715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 1521/1521 [02:35<00:00,  9.81it/s, Loss=1.88, Acc=18.08%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 3 | Loss: 1.9839 | Accuracy: 0.1808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 1521/1521 [02:35<00:00,  9.76it/s, Loss=1.99, Acc=17.88%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 4 | Loss: 1.9843 | Accuracy: 0.1788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 1521/1521 [02:38<00:00,  9.59it/s, Loss=2.13, Acc=17.71%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 5 | Loss: 1.9838 | Accuracy: 0.1771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 1521/1521 [02:35<00:00,  9.80it/s, Loss=2.12, Acc=18.17%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 6 | Loss: 1.9794 | Accuracy: 0.1817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 1521/1521 [02:34<00:00,  9.82it/s, Loss=1.88, Acc=18.21%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 7 | Loss: 1.9788 | Accuracy: 0.1821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 1521/1521 [02:35<00:00,  9.78it/s, Loss=1.92, Acc=18.48%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 8 | Loss: 1.9773 | Accuracy: 0.1848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 1521/1521 [02:35<00:00,  9.79it/s, Loss=1.97, Acc=18.29%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 9 | Loss: 1.9789 | Accuracy: 0.1829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 1521/1521 [02:34<00:00,  9.84it/s, Loss=1.99, Acc=18.49%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Epoch 10 | Loss: 1.9775 | Accuracy: 0.1849\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📂 Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📦 Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "import joblib\n",
        "import librosa\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# 💻 Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 🧠 Load AffectNet (ResNet18)\n",
        "affectnet_model = torch.hub.load('pytorch/vision:v0.14.0', 'resnet18', pretrained=False)\n",
        "affectnet_model.fc = nn.Linear(affectnet_model.fc.in_features, 8)\n",
        "affectnet_model.load_state_dict(torch.load('/content/drive/MyDrive/affectnet_resnet18.pth'))\n",
        "affectnet_model.to(device)\n",
        "affectnet_model.eval()\n",
        "\n",
        "# 🎧 Load RAVDESS Random Forest Model\n",
        "ravdess_model = joblib.load('/content/drive/MyDrive/ravdess_emotion_model.pkl')\n",
        "\n",
        "# 📊 Dummy RAVDESS embedding function (replace with yours if needed)\n",
        "def get_ravdess_embedding(filepath):\n",
        "    y, sr = librosa.load(filepath, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    mfccs = mfccs.mean(axis=1)\n",
        "    pred = ravdess_model.predict_proba([mfccs])[0]\n",
        "    return torch.tensor(pred, dtype=torch.float32)\n",
        "\n",
        "# 🔁 Fusion Model\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, affectnet_model, ravdess_model_embedding_size=8, fusion_hidden_dim=128, num_classes=8):\n",
        "        super(FusionModel, self).__init__()\n",
        "        self.affectnet_model = affectnet_model\n",
        "        self.ravdess_embedding_size = ravdess_model_embedding_size\n",
        "\n",
        "        for param in self.affectnet_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.affectnet_model.fc = nn.Identity()\n",
        "\n",
        "        self.ravdess_fc = nn.Sequential(\n",
        "            nn.Linear(ravdess_model_embedding_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512 + 64, fusion_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(fusion_hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, source, x):\n",
        "        if source == 'affectnet':\n",
        "            emb = self.affectnet_model(x)\n",
        "            audio_emb = torch.zeros(x.size(0), self.ravdess_embedding_size).to(x.device)\n",
        "        else:\n",
        "            emb = torch.zeros(x.size(0), 512).to(x.device)\n",
        "            audio_emb = x\n",
        "\n",
        "        audio_emb = self.ravdess_fc(audio_emb)\n",
        "        fused = torch.cat((emb.view(emb.size(0), -1), audio_emb.view(audio_emb.size(0), -1)), dim=1)\n",
        "        return self.fusion_fc(fused)\n",
        "\n",
        "# 🚀 Init Fusion Model\n",
        "fusion_model = FusionModel(affectnet_model).to(device)\n",
        "\n",
        "# 🧠 Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=1e-4)\n",
        "num_epochs = 10\n",
        "\n",
        "# ⚠️ Requires: fusion_loader defined externally\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    fusion_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    progress_bar = tqdm(fusion_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for source, datas, labels in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        labels = labels.to(device)\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(len(source)):\n",
        "            if source[i] == 'affectnet':\n",
        "                img = datas[i].to(device)\n",
        "                out = fusion_model('affectnet', img.unsqueeze(0))\n",
        "            else:\n",
        "                emb = get_ravdess_embedding(datas[i]).to(device)\n",
        "                out = fusion_model('ravdess', emb.unsqueeze(0))\n",
        "            outputs.append(out)\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=0)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item(), \"Acc\": f\"{100 * correct / total:.2f}%\"})\n",
        "\n",
        "    avg_loss = epoch_loss / len(fusion_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"\\n✅ Epoch {epoch+1} | Loss: {avg_loss:.4f} | Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "# 📈 Plot Training Loss\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss per Epoch')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 📊 Evaluation Function\n",
        "def evaluate_fusion_model(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for source, datas, labels in tqdm(loader, desc=\"Evaluating\"):\n",
        "            labels = labels.to(device)\n",
        "            batch_preds = []\n",
        "\n",
        "            for i in range(len(source)):\n",
        "                if source[i] == 'affectnet':\n",
        "                    img = datas[i].to(device)\n",
        "                    out = model('affectnet', img.unsqueeze(0))\n",
        "                else:\n",
        "                    emb = get_ravdess_embedding(datas[i]).to(device)\n",
        "                    out = model('ravdess', emb.unsqueeze(0))\n",
        "                batch_preds.append(out)\n",
        "\n",
        "            outputs = torch.cat(batch_preds, dim=0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    print(\"\\n📊 Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# ⚠️ Requires: fusion_test_loader defined externally\n",
        "# evaluate_fusion_model(fusion_model, fusion_test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "43_s8CgIioAb",
        "outputId": "65ecae47-991d-4262-d312-96c92eb701eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.14.0\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/affectnet/images/image2.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a3118ec8fea7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b6913f5fe284>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'affectnet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/affectnet/images/image2.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jytQAdejiVz3"
      },
      "outputs": [],
      "source": [
        "# 📂 Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📦 Imports and Device Setup\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from PIL import Image\n",
        "import librosa\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 🎯 Load Pretrained AffectNet Model\n",
        "affectnet_model = torch.hub.load('pytorch/vision:v0.14.0', 'resnet18', pretrained=False)\n",
        "affectnet_model.fc = nn.Linear(affectnet_model.fc.in_features, 8)  # 8 emotion classes\n",
        "affectnet_model.load_state_dict(torch.load('/content/drive/MyDrive/affectnet_resnet18.pth'))\n",
        "affectnet_model.to(device)\n",
        "affectnet_model.eval()\n",
        "\n",
        "# 🎧 Load RAVDESS Random Forest Model\n",
        "ravdess_model = joblib.load('/content/drive/MyDrive/ravdess_emotion_model.pkl')\n",
        "\n",
        "# 🔁 Define Fusion Model\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, affectnet_model, ravdess_model_embedding_size=8, fusion_hidden_dim=128, num_classes=8):\n",
        "        super(FusionModel, self).__init__()\n",
        "        self.affectnet_model = affectnet_model\n",
        "        self.ravdess_embedding_size = ravdess_model_embedding_size\n",
        "        self.fusion_hidden_dim = fusion_hidden_dim\n",
        "\n",
        "        # Freeze AffectNet CNN backbone\n",
        "        for param in self.affectnet_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.affectnet_model.fc = nn.Identity()  # use features before classifier (512-d)\n",
        "\n",
        "        # Audio (RAVDESS) embedding projection\n",
        "        self.ravdess_fc = nn.Sequential(\n",
        "            nn.Linear(ravdess_model_embedding_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Fusion + classifier head\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512 + 64, fusion_hidden_dim),  # 512 from image, 64 from audio\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(fusion_hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, source, x):\n",
        "        if source == 'affectnet':\n",
        "            emb = self.affectnet_model(x)  # shape: [B, 512]\n",
        "            audio_emb = torch.zeros(x.size(0), self.ravdess_embedding_size).to(x.device)\n",
        "        else:\n",
        "            emb = torch.zeros(x.size(0), 512).to(x.device)\n",
        "            audio_emb = x  # shape: [B, 8]\n",
        "\n",
        "        audio_emb = self.ravdess_fc(audio_emb)  # shape: [B, 64]\n",
        "        fused = torch.cat((emb, audio_emb), dim=1)  # shape: [B, 576]\n",
        "        out = self.fusion_fc(fused)  # shape: [B, 8]\n",
        "        return out\n",
        "\n",
        "# 🚀 Instantiate the Fusion Model\n",
        "fusion_model = FusionModel(affectnet_model, ravdess_model_embedding_size=8)\n",
        "fusion_model = fusion_model.to(device)\n",
        "print(f\"✅ FusionModel initialized on device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msKT8dQsiZPO"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "fusion_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    batch_count = 0\n",
        "    pbar = tqdm(fusion_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for source, datas, label in pbar:\n",
        "        label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = []\n",
        "        for i in range(len(source)):\n",
        "            if source[i] == 'affectnet':\n",
        "                data = datas[i].to(device)\n",
        "                out = fusion_model('affectnet', data.unsqueeze(0))\n",
        "            else:\n",
        "                mfcc = datas[i]\n",
        "                emb = get_ravdess_embedding(mfcc).to(device)\n",
        "                out = fusion_model('ravdess', emb)\n",
        "            outputs.append(out)\n",
        "        outputs = torch.cat(outputs, dim=0)\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "        pbar.set_postfix(loss=running_loss / batch_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYJFF1oZr5sQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCZngMm0i3rT"
      },
      "outputs": [],
      "source": [
        "torch.save(fusion_model.state_dict(), '/content/drive/MyDrive/fusion_model_final.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EnKHnGfz3WQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ab76be-38f8-46fd-e035-66e4db147069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.14.0\" to /root/.cache/torch/hub/v0.14.0.zip\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FusionModel initialized on device: cuda\n"
          ]
        }
      ],
      "source": [
        "# 📂 Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📦 Imports and Device Setup\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from PIL import Image\n",
        "import librosa\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 🎯 Load Pretrained AffectNet Model\n",
        "affectnet_model = torch.hub.load('pytorch/vision:v0.14.0', 'resnet18', pretrained=False)\n",
        "affectnet_model.fc = nn.Linear(affectnet_model.fc.in_features, 8)  # 8 emotion classes\n",
        "affectnet_model.load_state_dict(torch.load('/content/drive/MyDrive/affectnet_resnet18.pth'))\n",
        "affectnet_model.to(device)\n",
        "affectnet_model.eval()\n",
        "\n",
        "# 🎧 Load RAVDESS Random Forest Model\n",
        "ravdess_model = joblib.load('/content/drive/MyDrive/ravdess_emotion_model.pkl')\n",
        "\n",
        "# 🔁 Define Fusion Model\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, affectnet_model, ravdess_model_embedding_size=8, fusion_hidden_dim=128, num_classes=8):\n",
        "        super(FusionModel, self).__init__()\n",
        "        self.affectnet_model = affectnet_model\n",
        "        self.ravdess_embedding_size = ravdess_model_embedding_size\n",
        "        self.fusion_hidden_dim = fusion_hidden_dim\n",
        "\n",
        "        # Freeze AffectNet CNN backbone\n",
        "        for param in self.affectnet_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.affectnet_model.fc = nn.Identity()  # use features before classifier (512-d)\n",
        "\n",
        "        # Audio (RAVDESS) embedding projection\n",
        "        self.ravdess_fc = nn.Sequential(\n",
        "            nn.Linear(ravdess_model_embedding_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Fusion + classifier head\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(512 + 64, fusion_hidden_dim),  # 512 from image, 64 from audio\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(fusion_hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, source, x):\n",
        "        if source == 'affectnet':\n",
        "            emb = self.affectnet_model(x)  # shape: [B, 512]\n",
        "            audio_emb = torch.zeros(x.size(0), self.ravdess_embedding_size).to(x.device)\n",
        "        else:\n",
        "            emb = torch.zeros(x.size(0), 512).to(x.device)\n",
        "            audio_emb = x  # shape: [B, 8]\n",
        "\n",
        "        audio_emb = self.ravdess_fc(audio_emb)  # shape: [B, 64]\n",
        "\n",
        "        # Flatten to ensure 2D tensors before concatenation\n",
        "        emb = emb.view(emb.size(0), -1)          # [B, 512]\n",
        "        audio_emb = audio_emb.view(audio_emb.size(0), -1)  # [B, 64]\n",
        "\n",
        "        fused = torch.cat((emb, audio_emb), dim=1)  # shape: [B, 576]\n",
        "        out = self.fusion_fc(fused)  # shape: [B, 8]\n",
        "        return out\n",
        "\n",
        "# 🚀 Instantiate the Fusion Model\n",
        "fusion_model = FusionModel(affectnet_model, ravdess_model_embedding_size=8)\n",
        "fusion_model = fusion_model.to(device)\n",
        "print(f\"✅ FusionModel initialized on device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KXteY4fMijDQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "90172345-794b-4e2b-92a4-6aab8323a1ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-aaa4ffb15c7d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfusion_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    fusion_model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(fusion_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for source, datas, label in progress_bar:\n",
        "        label = label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(len(source)):\n",
        "            if source[i] == 'affectnet':\n",
        "                data = datas[i].to(device)\n",
        "                output = fusion_model('affectnet', data.unsqueeze(0))\n",
        "            else:\n",
        "                emb = get_ravdess_embedding(datas[i]).to(device)\n",
        "                output = fusion_model('ravdess', emb.unsqueeze(0))\n",
        "            outputs.append(output)\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=0)\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    avg_loss = epoch_loss / len(fusion_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"\\n✅ Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 📈 Plot: Training Loss over Epochs\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss per Epoch')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVTU8wONll66"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKK1VsjymF9K"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "595kPbMxmIB1"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oj4oTkfpd5o"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}