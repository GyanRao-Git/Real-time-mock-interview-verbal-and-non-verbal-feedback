# ğŸ¯ Real-Time Mock Interview Feedback System

This project tackles the challenge of providing **real-time feedback during mock interviews** by analyzing **verbal and non-verbal cues** through emotion recognition. We developed an intelligent system that evaluates **facial expressions**, **vocal tone**, and **spoken content** to simulate human-like feedback for interviewees.

---

## ğŸ§  What We Built

We explored **three core modalities**, each using a dedicated model:

### ğŸ‘ï¸ Visual Modality
A **ResNet18** model trained on the **AffectNet** dataset to classify facial emotions.  
âœ… **Achieved 69% accuracy**

### ğŸ”Š Audio Modality
A **Random Forest** classifier trained on **RAVDESS** audio samples using MFCC features to detect speech-based emotions.  
âœ… **Achieved 63.9% accuracy**

### ğŸ—£ï¸ Text Modality
A **lightweight NLP model** using **MediaSum** data to assess summary quality in terms of **sentiment**, **keyword coverage**, and **fluency**.  
âœ… **Achieved 77% accuracy**

---

## ğŸ”€ Multimodal Fusion (What Didn't Work)

We also attempted to combine the above signals into a **multimodal fusion model**. Unfortunately, this approach underperformed:

âŒ **Multimodal Accuracy: 18.5%**

### ğŸ“‰ Why It Failed:
- ğŸ”„ **Asynchronous inputs** across modalities  
- â±ï¸ **Lack of temporal alignment or modeling**  
- ğŸ“‰ **Noisy fusion techniques** without end-to-end optimization

---

## ğŸ’¡ Key Takeaways

- âœ… **Unimodal models are more robust** for real-time mock interview settings
- ğŸ§© **Facial emotion recognition** provides insight into confidence and engagement
- ğŸ¤ **Audio emotion analysis** reflects tone, stress, and emotional state
- ğŸ“ **Text-based feedback** can assess the clarity and sentiment of verbal responses
