# ğŸ¯ Real-Time Mock Interview Feedback System

This project tackles the challenge of providing **real-time feedback during mock interviews** by analyzing **verbal and non-verbal cues** through emotion recognition. We developed an intelligent system that evaluates **facial expressions**, **vocal tone**, and **spoken content** to simulate human-like feedback for interviewees.

---

## ğŸ§  What We Built

We explored **three core modalities**, each using a dedicated model:

### ğŸ‘ï¸ Visual Modality

A **ResNet18** model trained on the **AffectNet** dataset to classify facial emotions.  
âœ… **Achieved 69% accuracy**

### ğŸ”Š Audio Modality

A **Random Forest** classifier trained on **RAVDESS** audio samples using MFCC features to detect speech-based emotions.  
âœ… **Achieved 63.9% accuracy**

### ğŸ—£ï¸ Text Modality

A **lightweight NLP model** using **MediaSum** data to assess summary quality in terms of **sentiment**, **keyword coverage**, and **fluency**.  
âœ… **Achieved 77% accuracy**

---

## ğŸ”€ Multimodal Fusion (What Didn't Work)

We also attempted to combine the above signals into a **multimodal fusion model**. Unfortunately, this approach significantly underperformed:

âŒ **Multimodal Accuracy: 18.5%**

### ğŸ“‰ Why It Failed

- ğŸ”„ **Asynchronous inputs** across modalities (e.g., audio vs. visual timing)
- â±ï¸ **Lack of temporal modeling**, which is essential for interview dynamics
- ğŸ“‰ **Noisy and unoptimized fusion strategy** (not end-to-end trainable)
- ğŸ§© **Multimodal fusion complexity** â€” merging image features (AffectNet) and audio features (RAVDESS) is non-trivial, especially when one modality dominates or data distributions are mismatched
- ğŸ§Š **Frozen ResNet18 backbone** â€” convolutional layers were not fine-tuned, limiting adaptability to our dataset
- âš–ï¸ **Data imbalance** â€” some emotion classes had significantly fewer samples, leading to biased learning
- ğŸ§ª **Small batch size (8)** â€” may have caused unstable gradients or slowed convergence
- ğŸ”Š **Low-quality audio features** â€” MFCC-based embeddings from RAVDESS might lack discriminative power or be too noisy

## ğŸ’¡ Key Takeaways

- âœ… **Unimodal models are more robust** for real-time mock interview settings
- ğŸ§© **Facial emotion recognition** provides insight into confidence and engagement
- ğŸ¤ **Audio emotion analysis** reflects tone, stress, and emotional state
- ğŸ“ **Text-based feedback** can assess the clarity and sentiment of verbal responses
